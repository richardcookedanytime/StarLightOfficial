#!/usr/bin/env python3
"""
ç®€åŒ–çš„é«˜çº§è¯­æ³•åˆ†æå™¨æµ‹è¯•
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from compiler.lexer import Lexer
from compiler.advanced_parser import AdvancedParser

def test_basic_parsing():
    """æµ‹è¯•åŸºæœ¬è§£æåŠŸèƒ½"""
    print("=== æµ‹è¯•åŸºæœ¬è§£æ ===")
    
    code = """
    func hello() -> string {
        return "Hello World";
    }
    """
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        print(f"è¯æ³•åˆ†ææˆåŠŸï¼Œç”Ÿæˆ {len(tokens)} ä¸ª token")
        
        parser = AdvancedParser(tokens)
        ast = parser.parse()
        print(f"è¯­æ³•åˆ†ææˆåŠŸï¼Œç”Ÿæˆ {len(ast.statements)} ä¸ªè¯­å¥")
        
        if ast.statements:
            func = ast.statements[0]
            print(f"å‡½æ•°å: {func.name}")
        
        return True
    except Exception as e:
        print(f"âŒ åŸºæœ¬è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_async_function():
    """æµ‹è¯•å¼‚æ­¥å‡½æ•°"""
    print("\n=== æµ‹è¯•å¼‚æ­¥å‡½æ•° ===")
    
    code = """
    async func getData() -> string {
        return "data";
    }
    """
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        # æ‰“å° tokens æ¥è°ƒè¯•
        print("Tokens:")
        for i, token in enumerate(tokens[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {i}: {token}")
        
        parser = AdvancedParser(tokens)
        ast = parser.parse()
        print("âœ… å¼‚æ­¥å‡½æ•°è§£ææˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¼‚æ­¥å‡½æ•°è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_class():
    """æµ‹è¯•æ•°æ®ç±»"""
    print("\n=== æµ‹è¯•æ•°æ®ç±» ===")
    
    code = """
    data User {
        name: string;
        age: int;
    }
    """
    
    try:
        lexer = Lexer(code)
        tokens = lexer.tokenize()
        
        print("Tokens:")
        for i, token in enumerate(tokens[:15]):  # æ˜¾ç¤ºå‰15ä¸ª
            print(f"  {i}: {token}")
        
        parser = AdvancedParser(tokens)
        ast = parser.parse()
        print("âœ… æ•°æ®ç±»è§£ææˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®ç±»è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ Starlight é«˜çº§è¯­æ³•åˆ†æå™¨ç®€åŒ–æµ‹è¯•")
    print("=" * 50)
    
    success_count = 0
    total_tests = 3
    
    if test_basic_parsing():
        success_count += 1
    
    if test_async_function():
        success_count += 1
        
    if test_data_class():
        success_count += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    print("=" * 50)
